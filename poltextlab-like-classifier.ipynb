{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:17.978278Z",
     "start_time": "2024-04-11T11:39:52.371185Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "from torch import cuda\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_folder(folder_path):\n",
    "    dataframes = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jsonl.gz'):\n",
    "            with gzip.open(os.path.join(folder_path, filename), 'rt', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    json_data = json.loads(line)\n",
    "                    df = pd.DataFrame(json_data)\n",
    "                    dataframes.append(df)\n",
    "    if dataframes:\n",
    "        aggregated_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return aggregated_df\n",
    "    else:\n",
    "        print(\"No jsonl files found in the directory.\")\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:17.991193Z",
     "start_time": "2024-04-11T11:40:17.981372Z"
    }
   },
   "id": "f7ceefbe3e6811d0",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def reduce():\n",
    "    # csak a test\n",
    "    test = os.path.join(os.getcwd(), 'test')\n",
    "    df = read_folder(test)\n",
    "    df.drop('uuid', axis=1, inplace=True)\n",
    "    return df.groupby('major_topic_pred').apply(lambda x: x.sample(n=min(5, len(x)))).reset_index(drop=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:18.002728Z",
     "start_time": "2024-04-11T11:40:17.994357Z"
    }
   },
   "id": "757181da10fc77d1",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 9, 1: 8, 2: 18, 3: 12, 4: 15, 5: 17, 6: 13, 7: 1, 8: 5, 9: 20, 10: 4, 11: 23, 12: 21, 13: 10, 14: 2, 15: 3, 16: 14, 17: 19, 18: 16, 19: 7, 20: 999, 21: 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                title  \\\n44  Cáfolja a Frontex, hogy erőszakosak az uniós h...   \n36              Fontos szintek alá zuhant az olaj ára   \n83               Kedden rajtol a 77. Genfi Autószalon   \n51               Galambos Lajos: Nem vagyok szökésben   \n67  Kettő otthon, kettő az úton: folytatódik az NB...   \n78        A legmeghökkentőbb számítógép-konfigurációk   \n59              Így számítják ki a nyugdíjat 2014-ben   \n68  Fucsovics egy helyet javított, Medvegyev megel...   \n53                     Enyhítették Hunvald büntetését   \n1   Aki hátralép, veszíthet... – Szijjártó Péter n...   \n\n                                                 lead  \\\n44  Az európai uniós határ- és partvédelmi ügynöks...   \n36  Nagyot estek szerdán a világpiaci olajárak, íg...   \n83  A sajtó számára kedden nyitja meg kapuit Európ...   \n51  Nincs oka szökni vagy Mexikóban maradni, hisze...   \n67  A tizenkilencedik fordulóval folytatódnak az N...   \n78  A számítástechnika - és ezen belül a gépek - r...   \n59  2014-ben is a nettó átlagkeresettől és a szolg...   \n68  Fucsovics Márton egy helyet javítva múlt heti ...   \n53  A Kúrián folytatott eljárásban az ügyész a fel...   \n1   „Az útépítések, a digitális infrastruktúra fej...   \n\n                                              article        domain  \\\n44  A Frontex a dpa német hírügynökségnek e-mailbe...        hvg.hu   \n36  A szerdai esést nagyrészt az amerikai nyersola...  portfolio.hu   \n83  Idén március 8-án nyitja meg a kapuit a nagyér...      origo.hu   \n51  \"Nem vagyok szökésben, hiszen nincs ellenem ér...      origo.hu   \n67  2019. 03. 17., 15.00 óra\\n\\nIváncsa KSE (8.)– ...       feol.hu   \n78  A számítástechnika - és ezen belül a gépek - r...        hvg.hu   \n59  A nyugdíj alapjául szolgáló átlagkeresetnél 20...        hvg.hu   \n68  A rangsorban következő magyar játékos, Balázs ...      index.hu   \n53  Harmadfokú döntés született kedden Gál György ...         24.hu   \n1   Egyebek mellett erről is beszélt lapunknak teg...       zaol.hu   \n\n                                                  url     date_of_creation  \\\n44  https://hvg.hu/vilag/20190805_frontex_hatarors...  2019-08-05T20:26:00   \n36  https://www.portfolio.hu/uzlet/20181018/fontos...  2018-10-18T09:13:00   \n83  https://www.origo.hu/auto/20070303autocsodak.html  2007-03-05T10:57:00   \n51  https://www.origo.hu/teve/20160323-galambos-la...  2016-03-23T08:10:00   \n67  https://www.feol.hu/sport/helyi-sport/ketto-ot...  2019-03-15T21:32:00   \n78  https://hvg.hu/tudomany/20090921_furcsa_erdeke...  2009-09-22T05:14:00   \n59  https://hvg.hu/gazdasag/20131230_Igy_szamitjak...  2013-12-30T15:50:00   \n68  https://index.hu/sport/tenisz/2021/05/10/fucso...  2021-05-10T08:55:00   \n53  https://24.hu/kozelet/2014/07/01/enyhitettek-h...  2014-07-01T10:10:00   \n1   https://www.zaol.hu/gazdasag/aki-hatralep-vesz...  2018-04-07T08:00:00   \n\n                cc_date                tags  doc_similarity  major_topic_pred  \\\n44  2020-02-23T06:00:06             határőr        0.483312                 9   \n36  2019-11-19T08:48:58                 wti        0.604739                 8   \n83  2020-01-18T08:22:35  Ford Motor Company        0.688749                18   \n51  2021-09-22T09:05:33                Tévé        0.615171                12   \n67  2019-03-24T07:49:33         Iváncsa KSE        0.424394                15   \n78  2019-04-21T20:52:27        billentyűzet        0.697015                17   \n59  2019-09-15T10:58:07      szolgálati idő        0.841047                13   \n68  2021-06-15T21:56:42     novak djokovics        0.593206                15   \n53  2020-09-28T05:02:26             Közélet        0.400061                12   \n1   2020-08-08T17:56:29      helyi gazdaság        0.559537                 1   \n\n    major_topic_pred_index  \n44                       0  \n36                       1  \n83                       2  \n51                       3  \n67                       4  \n78                       5  \n59                       6  \n68                       4  \n53                       3  \n1                        7  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>lead</th>\n      <th>article</th>\n      <th>domain</th>\n      <th>url</th>\n      <th>date_of_creation</th>\n      <th>cc_date</th>\n      <th>tags</th>\n      <th>doc_similarity</th>\n      <th>major_topic_pred</th>\n      <th>major_topic_pred_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>44</th>\n      <td>Cáfolja a Frontex, hogy erőszakosak az uniós h...</td>\n      <td>Az európai uniós határ- és partvédelmi ügynöks...</td>\n      <td>A Frontex a dpa német hírügynökségnek e-mailbe...</td>\n      <td>hvg.hu</td>\n      <td>https://hvg.hu/vilag/20190805_frontex_hatarors...</td>\n      <td>2019-08-05T20:26:00</td>\n      <td>2020-02-23T06:00:06</td>\n      <td>határőr</td>\n      <td>0.483312</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Fontos szintek alá zuhant az olaj ára</td>\n      <td>Nagyot estek szerdán a világpiaci olajárak, íg...</td>\n      <td>A szerdai esést nagyrészt az amerikai nyersola...</td>\n      <td>portfolio.hu</td>\n      <td>https://www.portfolio.hu/uzlet/20181018/fontos...</td>\n      <td>2018-10-18T09:13:00</td>\n      <td>2019-11-19T08:48:58</td>\n      <td>wti</td>\n      <td>0.604739</td>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>Kedden rajtol a 77. Genfi Autószalon</td>\n      <td>A sajtó számára kedden nyitja meg kapuit Európ...</td>\n      <td>Idén március 8-án nyitja meg a kapuit a nagyér...</td>\n      <td>origo.hu</td>\n      <td>https://www.origo.hu/auto/20070303autocsodak.html</td>\n      <td>2007-03-05T10:57:00</td>\n      <td>2020-01-18T08:22:35</td>\n      <td>Ford Motor Company</td>\n      <td>0.688749</td>\n      <td>18</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>Galambos Lajos: Nem vagyok szökésben</td>\n      <td>Nincs oka szökni vagy Mexikóban maradni, hisze...</td>\n      <td>\"Nem vagyok szökésben, hiszen nincs ellenem ér...</td>\n      <td>origo.hu</td>\n      <td>https://www.origo.hu/teve/20160323-galambos-la...</td>\n      <td>2016-03-23T08:10:00</td>\n      <td>2021-09-22T09:05:33</td>\n      <td>Tévé</td>\n      <td>0.615171</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>Kettő otthon, kettő az úton: folytatódik az NB...</td>\n      <td>A tizenkilencedik fordulóval folytatódnak az N...</td>\n      <td>2019. 03. 17., 15.00 óra\\n\\nIváncsa KSE (8.)– ...</td>\n      <td>feol.hu</td>\n      <td>https://www.feol.hu/sport/helyi-sport/ketto-ot...</td>\n      <td>2019-03-15T21:32:00</td>\n      <td>2019-03-24T07:49:33</td>\n      <td>Iváncsa KSE</td>\n      <td>0.424394</td>\n      <td>15</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>A legmeghökkentőbb számítógép-konfigurációk</td>\n      <td>A számítástechnika - és ezen belül a gépek - r...</td>\n      <td>A számítástechnika - és ezen belül a gépek - r...</td>\n      <td>hvg.hu</td>\n      <td>https://hvg.hu/tudomany/20090921_furcsa_erdeke...</td>\n      <td>2009-09-22T05:14:00</td>\n      <td>2019-04-21T20:52:27</td>\n      <td>billentyűzet</td>\n      <td>0.697015</td>\n      <td>17</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>Így számítják ki a nyugdíjat 2014-ben</td>\n      <td>2014-ben is a nettó átlagkeresettől és a szolg...</td>\n      <td>A nyugdíj alapjául szolgáló átlagkeresetnél 20...</td>\n      <td>hvg.hu</td>\n      <td>https://hvg.hu/gazdasag/20131230_Igy_szamitjak...</td>\n      <td>2013-12-30T15:50:00</td>\n      <td>2019-09-15T10:58:07</td>\n      <td>szolgálati idő</td>\n      <td>0.841047</td>\n      <td>13</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>Fucsovics egy helyet javított, Medvegyev megel...</td>\n      <td>Fucsovics Márton egy helyet javítva múlt heti ...</td>\n      <td>A rangsorban következő magyar játékos, Balázs ...</td>\n      <td>index.hu</td>\n      <td>https://index.hu/sport/tenisz/2021/05/10/fucso...</td>\n      <td>2021-05-10T08:55:00</td>\n      <td>2021-06-15T21:56:42</td>\n      <td>novak djokovics</td>\n      <td>0.593206</td>\n      <td>15</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>Enyhítették Hunvald büntetését</td>\n      <td>A Kúrián folytatott eljárásban az ügyész a fel...</td>\n      <td>Harmadfokú döntés született kedden Gál György ...</td>\n      <td>24.hu</td>\n      <td>https://24.hu/kozelet/2014/07/01/enyhitettek-h...</td>\n      <td>2014-07-01T10:10:00</td>\n      <td>2020-09-28T05:02:26</td>\n      <td>Közélet</td>\n      <td>0.400061</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Aki hátralép, veszíthet... – Szijjártó Péter n...</td>\n      <td>„Az útépítések, a digitális infrastruktúra fej...</td>\n      <td>Egyebek mellett erről is beszélt lapunknak teg...</td>\n      <td>zaol.hu</td>\n      <td>https://www.zaol.hu/gazdasag/aki-hatralep-vesz...</td>\n      <td>2018-04-07T08:00:00</td>\n      <td>2020-08-08T17:56:29</td>\n      <td>helyi gazdaság</td>\n      <td>0.559537</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df = reduce()\n",
    "df = shuffle(reduced_df)\n",
    "labels = df['major_topic_pred'].unique().tolist()\n",
    "number_of_labels = len(labels)\n",
    "id_to_label = {_id: label for _id, label in enumerate(labels)}\n",
    "label_to_id = {label: _id for _id, label in enumerate(labels)}\n",
    "print(id_to_label)\n",
    "df['major_topic_pred_index'] = df['major_topic_pred'].map(lambda x: label_to_id[x])\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:19.896409Z",
     "start_time": "2024-04-11T11:40:18.004814Z"
    }
   },
   "id": "8ff4310eaad5b733",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=22, bias=True)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\",\n",
    "    num_labels=number_of_labels,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:23.127465Z",
     "start_time": "2024-04-11T11:40:19.900420Z"
    }
   },
   "id": "7ad0973ecdd045ce",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 27 28\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "size = df.shape[0]\n",
    "half = size // 2\n",
    "three_fourth = (3 * size) // 4\n",
    "train_texts = list(df['article'][:half])\n",
    "val_texts = list(df['article'][half:three_fourth])\n",
    "test_texts = list(df['article'][three_fourth:])\n",
    "print(len(train_texts), len(val_texts), len(test_texts))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:23.706133Z",
     "start_time": "2024-04-11T11:40:23.129474Z"
    }
   },
   "id": "9ba0cc96cea7cfaf",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 27 28\n"
     ]
    }
   ],
   "source": [
    "train_labels = list(df['major_topic_pred'][:half])\n",
    "val_labels = list(df['major_topic_pred'][half:three_fourth])\n",
    "test_labels = list(df['major_topic_pred'][three_fourth:])\n",
    "print(len(train_labels), len(val_labels), len(test_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:23.715921Z",
     "start_time": "2024-04-11T11:40:23.707700Z"
    }
   },
   "id": "cfa86895c3b17ffe",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DataLoader(Dataset):\n",
    "    def __init__(self, encodings, _labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = _labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:23.891522Z",
     "start_time": "2024-04-11T11:40:23.718055Z"
    }
   },
   "id": "1fc723147afce3c2",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_encodings, train_labels)\n",
    "val_dataloader = DataLoader(val_encodings, val_labels)\n",
    "test_dataset = DataLoader(test_encodings, test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:24.253103Z",
     "start_time": "2024-04-11T11:40:23.894888Z"
    }
   },
   "id": "dda9b996732e60e3",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./z',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    save_strategy='steps',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:24.278283Z",
     "start_time": "2024-04-11T11:40:24.256229Z"
    }
   },
   "id": "b2364a5b3b8f2fdf",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_metrics(predictions):\n",
    "    labels = predictions.label_ids\n",
    "    preds = predictions.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:40:24.304177Z",
     "start_time": "2024-04-11T11:40:24.280357Z"
    }
   },
   "id": "a303811a8485e051",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 999 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 8\u001B[0m\n\u001B[0;32m      1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m      2\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m      3\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics\n\u001B[0;32m      7\u001B[0m )\n\u001B[1;32m----> 8\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1780\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1778\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1780\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1781\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1782\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1783\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1784\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1785\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2118\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m   2117\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[1;32m-> 2118\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2121\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2122\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2123\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2124\u001B[0m ):\n\u001B[0;32m   2125\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2126\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3036\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs)\u001B[0m\n\u001B[0;32m   3033\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m   3035\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[1;32m-> 3036\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3038\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3039\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3059\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[1;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[0;32m   3057\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3058\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 3059\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3060\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[0;32m   3061\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[0;32m   3062\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1599\u001B[0m, in \u001B[0;36mBertForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1597\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mproblem_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msingle_label_classification\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1598\u001B[0m     loss_fct \u001B[38;5;241m=\u001B[39m CrossEntropyLoss()\n\u001B[1;32m-> 1599\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_fct\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1600\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mproblem_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulti_label_classification\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1601\u001B[0m     loss_fct \u001B[38;5;241m=\u001B[39m BCEWithLogitsLoss()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m-> 1179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1180\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1181\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3057\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3058\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3059\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: Target 999 is out of bounds."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataloader,\n",
    "    eval_dataset=val_dataloader,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T11:41:58.248558Z",
     "start_time": "2024-04-11T11:40:24.307186Z"
    }
   },
   "id": "e21cd72062895594",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, padding='True', truncation=True, max_length=512, return_tensors='pr')\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred_label_idx = probs.argmax()\n",
    "\n",
    "    pred_label = model.config.id2label[pred_label_idx.item()]\n",
    "    return probs, pred_label_idx, pred_label\n",
    "\n",
    "predict(\"Egyre t\\u00f6bb a t\\u00f6meges H1N1-megbetegedes...\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea89b166988dbc04"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path = 'poltextlab-like-classification-model'\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aa4efffff6031b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def reload():\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer=BertTokenizerFast.from_pretrained(model_path)\n",
    "    nlp = pipeline('sentiment analysis', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # nlp(...)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3db19494e37efe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "17166988ad439171"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
